\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{color}
% -------------  Header   ---------------
\usepackage{fancyhdr}

\pagestyle{fancy}

\fancyhf{}
\fancyheadoffset{2pt}
\fancyhead[L]{Algebra II}
\fancyhead[R]{Universidad Austral}

% -------------  Margenes ---------------
\addtolength{\oddsidemargin}{-.875in}
	\addtolength{\evensidemargin}{-.875in}
	\addtolength{\textwidth}{1.75in}

	\addtolength{\topmargin}{-.875in}
	\addtolength{\textheight}{1.75in}
% ------------- /Margenes ---------------
% ------------- Espaciado entre lineas ---------------
\renewcommand{\baselinestretch}{1.5} 
% !TeX spellcheck = es_ES
\title{Matriz Diagonalizable \\Autovectores}
\author{Álgebra II}
\date{\vspace{-5ex}}

\begin{document}
\maketitle{}
% ------------- Comienza el documento ---------------
\section{Diagonalización de Matriz de T.L.}
Dado A $\in {\rm I\!R}^{nxn} \Rightarrow$ siempre existe una \textit{f}: ${\rm I\!R}$$^{n}\rightarrow {\rm I\!R}^{n}$ / \textit{f}($\vec{x}$) = A $\cdot \vec{x}$. \\
En otras palabras, \textit{f} es una T.L. y \textbardbl \textit{f}\textbardbl $_{CC}$ = A
\subsection{Definición}
Decimos que A es una matriz diagonalizable si y sólo si, al definir \textit{f}: ${\rm I\!R}$$^{n}\rightarrow ${\rm I\!R}$^{n}$ / \textit{f}($\vec{x}$) = A $\cdot \vec{x}$ existe una base B tal que \textbardbl \textit{f}\textbardbl $_{BB}$ es una matriz diagonal, o sea existe una matriz P = \textbardbl \textit{id}\textbardbl $_{BC}$ / \textbardbl \textit{f}\textbardbl $_{B}$ = P$^{-1}\cdot A \cdot P$ \textit{(¿Por qué?)}
% ------------- Prop: Diagonalizable sii base B ---------------
\subsection{Proposición}
A es diagonalizable $\leftrightarrow$ existe una base B = \{$\vec{v}_{1}, \vec{v}_2,... \vec{v}_n $\} base de ${\rm I\!R}^{n}$ y escalares $\lambda_{i}$ tal que A$\cdot\vec{v}_{i}=\lambda_{i}\cdot\vec{v}_{i}$\\
$\Rightarrow$) {\bfseries \textcolor{blue}{Hipótesis:}} A es diagonalizable, o sea si \textit{f}: ${\rm I\!R}^{n}$ $\rightarrow$ ${\rm I\!R}^{n}$ / \textit{f}($\vec{x}$) = A $\cdot \vec{x}$ existe una base B tal que \textbardbl \textit{f}\textbardbl $_{BB}$ \indent es una matriz diagonal\\
\indent{\bfseries \textcolor{red}{Tesis:}} Existen escalares $\lambda_{i}$ tal que A$\cdot\vec{v}_{i}$=$\lambda_{i}$ \\
\textbf{Demostración:} Como \textbardbl \textit{f}\textbardbl $_{BB}$ es diagonal $\Rightarrow$ \textbardbl \textit{f}\textbardbl $_{BB}$ = $M = \begin{pmatrix} a_{11}&0&\ldots&0\\ 0&a_{22}&\ldots&0\\ \ldots&\ldots&\ddots&\ldots\\ 0&0&\ldots&a_{nn} \end{pmatrix}$ \\
Para interpretar esta matriz, recordamos la definición de matriz de T.L. dada una base:\\
\textit{f}($\vec{v}_{1}$) = $a_{11}\cdot\vec{v}_1 + 0\cdot\vec{v}_2 + \ldots 0\cdot\vec{v}_{n}$ = $a_{11}\cdot\vec{v}_1$\\
\textit{f}($\vec{v}_{2}$) = $0\cdot\vec{v}_1 + a_{22}\cdot\vec{v}_2 + \ldots 0\cdot\vec{v}_{n}$ = $a_{22}\cdot\vec{v}_2$\\
$\vdots$\\
\textit{f}($\vec{v}_{n}$) = $0\cdot\vec{v}_1 + 0\cdot\vec{v}_2 + \ldots a_{nn}\cdot\vec{v}_{n}$ = $a_{nn}\cdot\vec{v}_n$ \\
Con lo que queda demostrada la tesis. Veamos ahora la otra implicación.
\newpage
$\Leftarrow$) {\bfseries \textcolor{blue}{Hipótesis:}} $\exists$ base B = \{$\vec{v}_{1}, \vec{v}_2,... \vec{v}_n $\} base de $R^{n}$ y escalares $\lambda_{i}$ tal que A$\cdot\vec{v}_{i}$=$\lambda_{i}$\\
\indent \indent {\bfseries \textcolor{red}{Tesis:}} A es diagonalizable \\
\textbf{Demostración:} Debemos probar que si \textit{f}: ${\rm I\!R}^{n}\rightarrow ${\rm I\!R}$^{n}$ / \textit{f}($\vec{x}$) = A $\cdot \vec{x}$ entonces \textbardbl \textit{f}\textbardbl $_{BB}$ es diagonal. \\
Para esto, armemos \textbardbl \textit{f}\textbardbl $_{BB}$ :\\
\textit{f}($\vec{v}_{1}$) = A$\cdot\vec{v}_1$ = $\lambda_{1}\cdot\vec{v}_{1}$ $\Rightarrow$ la 1ra columna de \textbardbl \textit{f}\textbardbl $_{B}$ será $ \begin{pmatrix} \lambda_{1}\\ 0\\ \vdots\\ 0 \end{pmatrix}$\\
\textit{f}($\vec{v}_{2}$) = A$\cdot\vec{v}_2$ = $\lambda_{2}\cdot\vec{v}_{2}$ $\Rightarrow$ la 2da columna de \textbardbl \textit{f}\textbardbl $_{B}$ será $ \begin{pmatrix} 0 \\ \lambda_{2}\\ \vdots\\ 0 \end{pmatrix}$\\
$\vdots$\\
\textit{f}($\vec{v}_{n}$) = A$\cdot\vec{v}_n$ = $\lambda_{n}\cdot\vec{v}_{n}$ $\Rightarrow$ la n columna de \textbardbl \textit{f}\textbardbl $_{B}$ será $ \begin{pmatrix} 0 \\ 0\\ \vdots\\ \lambda_n \end{pmatrix}$\\
Por lo tanto, \textbardbl \textit{f}\textbardbl $_{B}$ = $ \begin{pmatrix} a_{11}&0&\ldots&0\\ 0&a_{22}&\ldots&0\\ \ldots&\ldots&\ddots&\ldots\\ 0&0&\ldots&a_{nn} \end{pmatrix}$ que es una matriz diagonal. \\
\newpage
% ------------- Autovalores y Autovectores ---------------
\section{Autovalores y Autovectores}
Dada una matriz A $\in {\rm I\!R}^{nxn}$ llamaremos autovalor de A a un número $\lambda \in {\rm I\!R}$ si existe un vector $\vec{v} \in {\rm I\!R}^n, \vec{v} \neq \vec{0}$ al que llamaremos autovector de A asociado al autovalor $\lambda$ tal que $A\cdot\vec{v} = \lambda\cdot\vec{v}$
\subsection{Proposiciones}
% ------------- Proposición EZPZ ---------------
\subsubsection{C.L. vectores asociados}
Si $\vec{v}_1, \vec{v}_2, \ldots \vec{v}_n$ son autovectores de una matriz A asociados a un autovalor $\lambda$, entonces cualquier combinación lineal de ellos será autovector de A asociado a $\lambda$. \\
\textbf{Demostración:} Sabiendo que A$\cdot\vec{v}_i = \lambda\cdot\vec{v}_i$\\ 
A$\cdot( \alpha_1\vec{v}_1 + \alpha_2\vec{v}_2 + \ldots \alpha_n\vec{v}_n )$ $\rightarrow$ Planteo C.L. de los vectores\\ 
$ \alpha_1A\vec{v}_1 + \alpha_2A\vec{v}_2 + \ldots \alpha_nA\vec{v}_n$ $\rightarrow$ Distribuyo A\\
$ \alpha_1\lambda\vec{v}_1 + \alpha_2\lambda\vec{v}_2 + \ldots \alpha_n\lambda\vec{v}_n$  $\rightarrow$ Uso hipótesis\\
$\lambda \cdot(\alpha_1\vec{v}_1 + \alpha_2\vec{v}_2 + \ldots \alpha_n\vec{v}_n)$ $\rightarrow$ Factor Común $\lambda$\\
Por lo tanto A$\cdot( \alpha_1\vec{v}_1 + \alpha_2\vec{v}_2 + \ldots \alpha_n\vec{v}_n )$ = $\lambda \cdot(\alpha_1\vec{v}_1 + \alpha_2\vec{v}_2 + \ldots \alpha_n\vec{v}_n)$
\subsubsection{Relación entre autovalor y determinante}
$\lambda$ es un autovalor asociado a $\vec{v}$, autovector de una matriz A $\Leftrightarrow$ \textbar  A - $\lambda$I\textbar = 0\\
{\bfseries \textcolor{blue}{Hipótesis:}} A $\in {\rm I\!R}^{nxn}$, $\lambda$ y $\vec{v}$ autovalor y autovector de A tal que $A\cdot\vec{v} = \lambda\cdot\vec{v}$ siendo $\vec{v} \neq \vec{0}$ (¿Por qué?)\\
{\bfseries \textcolor{red}{Tesis:}} \textbar  A - $\lambda$I\textbar = 0\\
\textbf{Demostración:} En $A\cdot\vec{v} = \lambda\cdot\vec{v}$ con  $\vec{v} \neq \vec{0}$, $\vec{v}$ es solución del sistema homogéneo (A - $\lambda$I)$\cdot\vec{x}$ = $\vec{0}$ (¿Por qué?).\\
Luego este es un sistema compatible indeterminado: por lo tanto el determinante de su matriz es = 0. Es decir, \textbar  A - $\lambda$I\textbar = 0\\
La otra implicación queda a cargo de ustedes ,
pues es muy parecida a ésta
\subsubsection{Concepto Geométrico}
\textit{f}: Dada ${\rm I\!R}^{2}\rightarrow {\rm I\!R}^{2}$ o \textit{f}: ${\rm I\!R}^{3}\rightarrow {\rm I\!R}^{3}$ podemos decir que $\vec{x}$ es autovector de \textit{f} si al transformarse conserva la misma dirección.\\
Ejemplos:\\
a) \textit{f}: ${\rm I\!R}^{2}\rightarrow {\rm I\!R}^{2}$ / \textit{f}($x,y$) = ($x, -y$)\\
b)\textit{f}: ${\rm I\!R}^{3}\rightarrow {\rm I\!R}^{3}$ / \textit{f}($x,y,z$) = ($x+y, x-y, 0$)\\
Luego halla los autovalores y autovectores de A y verifica lo que dice el enunciado anterior.\\
\newpage
\subsection{Propiedades de los Autovectores}
\textbf{1)} S = \{$\vec{x}$ / $\vec{x} = \vec{0}$ o $\vec{x}$ es un autovector asociado al autovalor $\lambda$ \} es un subespacio. (Queda como ejercicio)\\
% ------------- Autovectores distintos son L.I. ---------------
\textbf{2)} Los autovectores correspondientes a autovalores distintos son L.I.\\
{\bfseries \textcolor{blue}{Hipótesis:}} A $\in {\rm I\!R}^{nxn}$, \{$\vec{x}_{1}, \vec{x}_2,... \vec{x}_n $\} autovectores de A . $\lambda_{1}, \lambda_{2},\ldots \lambda_{n}$ sus autovalores asociados tales que $\lambda_i \neq \lambda_j$ $\forall$ $i\neq j$ \\
{\bfseries \textcolor{red}{Tesis:}} \{$\vec{x}_{1}, \vec{x}_2,... \vec{x}_n $\} es un conjunto L.I.\\
\textbf{Demostración:} La demostración habría que hacerla usando el principio de inducción completa. Nosotros vamos a probar que vale para n=2 y luego usando que vale para n=2 demostraremos que vale para n=3 y así sucesivamente se puede demostrar para cualquier valor de n.\\
a) Probaremos para $n=2$, es decir que \{$\vec{x}_{1}, \vec{x}_2$\} es un conjunto L.I. \\
Sea: 
\begin{align}
\alpha_1\vec{x}_1+\alpha_2\vec{x}_2 = \vec{0}
\end{align}
Multiplicamos ambos miembros por A y nos queda: $\alpha_{1}A\vec{x}_1+\alpha_{2}A\vec{x}_2 = \vec{0}$\\
Pero $A\vec{x}_1 = \lambda_1\vec{x}_1$ y $A\vec{x}_2 = \lambda_2\vec{x}_2$ $\rightarrow$ nos queda:
\begin{align}
\alpha_1\lambda_1\vec{x}_1+\alpha_2\lambda_2\vec{x}_2 = \vec{0}
\end{align}
Las incógnitas son entonces $\alpha_1$ y $\alpha_2$, las cuales debemos despejar de (1) y (2).
\begin{itemize}
\item Multiplico (1) por $\lambda_1$: $\alpha_1\lambda_1\vec{x}_1+\alpha_2\lambda_1\vec{x}_2 = \vec{0}$ 
\item Le resto la ecuación (2): $\alpha_2\lambda_1\vec{x}_2 - \alpha_2\lambda_2\vec{x}_2=\vec{0}$
\end{itemize}
Nos queda entonces $\alpha_2\vec{x}_2\cdot(\lambda_1-\lambda_2)=\vec{0}$\\
Pero $\vec{x}_2 \neq \vec{0}$ y $(\lambda_1 - \lambda_2) \neq \vec{0}$ (¿Por qué?). Por lo tanto $\alpha_2=0$\\
Reemplazando en (1) nos queda $\alpha_2=0$. Hemos probado que \{$\vec{x}_{1}, \vec{x}_2$\} es un conjunto L.I. \\
b) Veamos ahora para $n=3$ o sea tenemos $\vec{x}_{1}, \vec{x}_2, \vec{x}_3$ autovectores de A y $\lambda_1, \lambda_2, \lambda_3$ diferentes entre sí. Queremos mostrar que \{$\vec{x}_{1}, \vec{x}_2, \vec{x}_3$\} es un conjunto L.I.\\
Sea: 
\begin{align}
\alpha_1\vec{x}_1+\alpha_2\vec{x}_2+\alpha_3\vec{x}_3 = \vec{0}
\end{align}
Multiplicando por A y reemplazando por los autovectores nos queda:
\begin{align}
\alpha_{1}\lambda_1\vec{x}_1+\alpha_{2}\lambda_2\vec{x}_2 + \alpha_{3}\lambda_3\vec{x}_3 = \vec{0}
\end{align}
Hacemos $\lambda_1\cdot$(3)-(4) y nos queda $\alpha_2(\lambda_1-\lambda_2)\vec{x}_2+\alpha_3(\lambda_1-\lambda_3)\vec{x}_3=\vec{0}$ \\
Como $\vec{x}_2$ y $\vec{x}_3$ son 2 autovectores asociados a autovalores diferentes son L.I. Luego si tengo una combinación lineal de ellos igualada a $\vec{0}$ sos escalares son igual a 0.\\
$\alpha_2(\lambda_1-\lambda2)=0$ ; $\alpha_3(\lambda_1-\lambda_3)=0$ $\rightarrow$ $\alpha_2 = \alpha_3 = 0$ (¿Por qué?)\\
Reemplazando en (3) nos queda $\alpha_1=0$\\
Entonces hemos probado que \{$\vec{x}_{1}, \vec{x}_2, \vec{x}_3$\} es un conjunto L.I.
\newpage
\subsection{Consecuencias}
\begin{enumerate}
\item A $\in \rm {I\!R}^{nxn}$ es una matriz diagonalizable $\Leftrightarrow$ \\
a) Existe B = \{$\vec{v}_{1}, \vec{v}_2,... \vec{v}_n $\} base de $\in \rm {I\!R}^{n}$ formada por autovectores \\
b) Tiene n autovectores L.I.
\item Si A tiene n autovalores diferentes $\rightarrow$ A es diagonalizable (No vale el recíproco)
\item Si A tiene algun autovalor múltiple ($\lambda_0$) pero la dim$(S)_{\lambda_0}$ = $multiplicidad_{\lambda_0}$ $\rightarrow $ A es diagonalizable (No vale el recíproco)
\end{enumerate}
\section{Diagonalización de matrices simétricas}
\begin{enumerate}
\item Las matrices simétricas son siempre diagonalizables \textit{(sin demostración)}
\item Los autovectores de matrices simétricas correspondientes a autovalores diferentes son ortogonales 

{\bfseries \textcolor{blue}{Hipótesis:}} A $\in {\rm I\!R}^{nxn}$ matriz simétrica, \{$\vec{x}_{1}, \vec{x}_2$\} autovectores de A asociados a $\lambda_{1}, \lambda_{2}$ donde $\lambda_{1} \neq \lambda_{2}$\\
{\bfseries \textcolor{red}{Tesis:}} $\vec{x}_{1} \perp \vec{x}_2$ es decir $(\vec{x}_1\cdot\vec{x}_2)=0$\\
\textbf{Demostración:} Consideramos el producto interno canónico en el que se cumple que $(\vec{x}\cdot\vec{y}) = \vec{x}^\mathsf{T}\cdot\vec{y}$\\
Hacemos \\
$(\vec{x}_1)^\mathsf{T}A\cdot\vec{x}_2=(\vec{x}_1)^\mathsf{T}\lambda_2\vec{x}_2=\lambda_2(\vec{x}_1\cdot\vec{x}_2)$\\
$(\vec{x}_1)^\mathsf{T}A \cdot \vec{x}_2=(\vec{x}_1)^\mathsf{T} A^\mathsf{T} \vec{x}_2 =(A\cdot\vec{x}_1)^\mathsf{T}\vec{x}_2=\lambda_1(\vec{x}_1\cdot\vec{x}_2)$\\
Podemos igualar ambas igualdades finales dado que parten de la misma expresión:\\
$\lambda_2\cdot(\vec{x}_1\cdot\vec{x}_2)=\lambda_1\cdot(\vec{x}_1\cdot\vec{x}_2)$, luego pasamos restando y factor común: \\
$(\lambda_2-\lambda_1)(\vec{x}_1\cdot\vec{x}_2)=0$ y como $\lambda_1 \neq \lambda_2 \rightarrow (\vec{x}_1\cdot\vec{x}_2)=0$
% ------------- Simetrica sii base ortogonal ---------------
\item A es una matriz simétrica $\leftrightarrow$ tiene una base ortogonal de autovectores\\
$\rightarrow$){\bfseries \textcolor{blue}{Hipótesis:}} A es simétrica\\
{\bfseries \textcolor{red}{Tesis:}} $\exists$ una base ortonormal de ${\rm I\!R}^{n}$ formada por autovectores\\
\textbf{Demostración:} Siendo A simétrica consideraremos 2 casos posibles:\\
a) Tiene n autovalores distintos, los autovectores asociados serían ortogonales (probado anteriormente) y dividiendo cada uno por su norma tendremos una base ortonormal de autovectores \\
b) Hay algún autovalor múltiple ($\lambda_0$), como sabemos que es diagonalizable, la Dim$(S)_{\lambda_{0}}$ = multiplicidad de $\lambda_0$ $\rightarrow$ dentro de ese subespacio podré encontrar una base ortonormal y junto con los otros formarán una base ortonormal de ${\rm I\!R}^{n}$ formada por autovectores

$\leftarrow$){\bfseries \textcolor{blue}{Hipótesis:}} A tiene una base ortonormal de autovectores\\
{\bfseries \textcolor{red}{Tesis:}} A es simétrica\\
\textbf{Demostración:} Como A tiene una base ortogonal de
autovectores $\rightarrow$\\
$\exists$ D diagonal tal que $A = P \cdot D \cdot P ^\mathsf{T} $\\
Por lo tanto $A^\mathsf{T}=(P \cdot D \cdot P ^\mathsf{T})^\mathsf{T}= (P^\mathsf{T})^\mathsf{T} \cdot D^\mathsf{T} \cdot P ^\mathsf{T} = P \cdot D \cdot P ^\mathsf{T} = A$\\
Queda demostrado que A es simétrica
\end{enumerate}
\end{document}